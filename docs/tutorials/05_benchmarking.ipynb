{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking Behavior Planners in BARK\n",
    "\n",
    "This notebook shows the benchmarking workflow of BARK.\n",
    "\n",
    "Systematically benchmarking behavior consists of\n",
    "1. A reproducable set of scenarios (we call it **BenchmarkDatabase**)\n",
    "2. Metrics, which you use to study the performance (we call it **Evaluators**)\n",
    "3. The behavior model(s) under test\n",
    "\n",
    "Our **BenchmarkRunner** can then run the benchmark and produce the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "%matplotlib tk\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Video\n",
    "\n",
    "from benchmark_database.load.benchmark_database import BenchmarkDatabase\n",
    "from benchmark_database.serialization.database_serializer import DatabaseSerializer\n",
    "from bark.benchmark.benchmark_runner import BenchmarkRunner, BenchmarkConfig, BenchmarkResult\n",
    "from bark.benchmark.benchmark_analyzer import BenchmarkAnalyzer\n",
    "\n",
    "from bark.runtime.commons.parameters import ParameterServer\n",
    "\n",
    "from bark.runtime.viewer.matplotlib_viewer import MPViewer\n",
    "from bark.runtime.viewer.video_renderer import VideoRenderer\n",
    "\n",
    "\n",
    "from bark.core.models.behavior import BehaviorIDMClassic, BehaviorConstantAcceleration\n",
    "from bark.core.models.behavior import BehaviorModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database\n",
    "The benchmark database provides a reproducable set of scenarios.\n",
    "A scenario get's created by a ScenarioGenerator (we have a couple of them). The scenarios are serialized into binary files and packed together with the map file and the parameter files into a `.zip`-archive. We call this zipped archive a relase, which can be published at Github, or processed locally.\n",
    "\n",
    "## We will first start with the DatabaseSerializer\n",
    "\n",
    "The **DatabaseSerializer** recursively serializes all scenario param files sets\n",
    " within a folder.\n",
    " \n",
    "We will process the database directory from Github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bark.runtime.commons.parameters.ParameterServer object at 0x7fd46dae6710>\n",
      "maps/city_highway_straight.xodr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Testing scenario_set1 with seed 2000 from generator ConfigurableScenarioGeneration\n",
      "INFO:root:Running scenario 0 of 10 in set scenario_set1\n",
      "WARNING:root:Do not set down the number of serialized scenarios in a release                     -> set serialized_scenarios=None\n",
      "INFO:root:The following list of files will be released:\n",
      "INFO:root:/maps/city_highway_straight.xodr/city_highway_straight.xodr\n",
      "INFO:root:/scenario_sets/highway_merging/set_info_scenario_set1/set_info_scenario_set1\n",
      "INFO:root:/scenario_sets/highway_merging/scenario_set1_scenarios10_seed2000.bark_scenarios/scenario_set1_scenarios10_seed2000.bark_scenarios\n",
      "INFO:root:/scenario_sets/highway_merging/scenario_set1.json/scenario_set1.json\n",
      "INFO:root:Packed release file /home/esterle/.cache/bazel/_bazel_esterle/d337abac8c371120c1b9affa1049fa7e/execroot/bark_project/bazel-out/k8-fastbuild/bin/docs/tutorials/run.runfiles/benchmark_database/data/benchmark_database_tutorial.zip\n",
      "INFO:root:Assuming local release as you did not provide a github token.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: ../../../benchmark_database/data/benchmark_database_tutorial.zip\n"
     ]
    }
   ],
   "source": [
    "dbs = DatabaseSerializer(test_scenarios=1, test_world_steps=5, num_serialize_scenarios=10)\n",
    "dbs.process(\"../../../benchmark_database/data/tutorial_database\")\n",
    "local_release_filename = dbs.release(version=\"tutorial\")\n",
    "\n",
    "print('Filename:', local_release_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then reload to test correct parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:extracting zipped-database ../../../benchmark_database/data/benchmark_database_tutorial.zip to temporary directory ../tmp/bark_extracted_databases/5b5eb9c9-98cf-44ad-bf70-c5c64b6f0200\n",
      "INFO:root:Found info dict set_info_scenario_set1\n",
      "INFO:root:The following scenario sets are available\n",
      "INFO:root:\n",
      "                    GeneratorName  NumScenarios                                            Params    Seed                                         Serialized        SetName   SetParameters\n",
      "0  ConfigurableScenarioGeneration          10.0  scenario_sets/highway_merging/scenario_set1.json  2000.0  scenario_sets/highway_merging/scenario_set1_sc...  scenario_set1  {'Test1': 200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario:  <bark.runtime.scenario.scenario_generation.scenario_generation.ScenarioGeneration object at 0x7fd46ca43b90>\n"
     ]
    }
   ],
   "source": [
    "db = BenchmarkDatabase(database_root=local_release_filename)\n",
    "scenario_generation, _, _ = db.get_scenario_generator(scenario_set_id=0)\n",
    "\n",
    "for scenario_generation, _, _ in db:\n",
    "  print('Scenario: ', scenario_generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluators\n",
    "\n",
    "Evaluators allow to calculate a boolean, integer or real-valued metric based on the current simulation world state.\n",
    "\n",
    "The current evaluators available in BARK are:\n",
    "- StepCount: returns the step count the scenario is at.\n",
    "- GoalReached: checks if a controlled agentâ€™s Goal Definitionis satisfied.\n",
    "- DrivableArea: checks whether the agent is inside its RoadCorridor.\n",
    "- Collision(ControlledAgent): checks whether any agent or only the currently controlled agent collided\n",
    "- LTLEvaluator: checking traffic rules based on arbitrary LTL formulas\n",
    "\n",
    "Let's now map those evaluators to some symbols, that are easier to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluators = {\"success\" : \"EvaluatorGoalReached\", \\\n",
    "              \"collision\" : \"EvaluatorCollisionEgoAgent\", \\\n",
    "              \"max_steps\": \"EvaluatorStepCount\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define the terminal conditions of our benchmark. We state that a scenario ends, if\n",
    "- a collision occured\n",
    "- the number of time steps exceeds the limit\n",
    "- the definition of success becomes true (which we defined to reaching the goal, using EvaluatorGoalReached)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_when = {\"collision\" :lambda x: x, \\\n",
    "                 \"max_steps\": lambda x : x>50, \\\n",
    "                 \"success\" : lambda x: x}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behaviors Under Test\n",
    "Let's now define the Behaviors we want to compare. We will compare IDM with Constant Velocity, but we could also compare two different parameter sets for IDM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ParameterServer()\n",
    "\n",
    "#params[\"BehaviorIDMClassic\"][\"AccelerationLowerBound\"] = -0.1\n",
    "\n",
    "behaviors_tested = {\"IDM\": BehaviorIDMClassic(params), \\\n",
    "                    \"Const\" : BehaviorConstantAcceleration(params)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Runner\n",
    "\n",
    "The BenchmarkRunner allows to evaluate behavior models with different parameter configurations over the entire benchmarking database. \n",
    "\n",
    "Technically, the benchmark runner will run all configs.\n",
    "\n",
    "A config is basically a simulation run, where step size, controlled agent, terminal conditions and metrics have been defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:BenchmarkRunner:Total number of 20 configs to run\n",
      "INFO:BenchmarkRunner:Running config idx 0 being 0/19: Scenario 0 of set \"scenario_set1\" for behavior \"IDM\"\n",
      "INFO:BenchmarkRunner:Running config idx 1 being 1/19: Scenario 1 of set \"scenario_set1\" for behavior \"IDM\"\n",
      "INFO:BenchmarkRunner:Running config idx 2 being 2/19: Scenario 2 of set \"scenario_set1\" for behavior \"IDM\"\n",
      "INFO:BenchmarkRunner:Running config idx 3 being 3/19: Scenario 3 of set \"scenario_set1\" for behavior \"IDM\"\n",
      "INFO:BenchmarkRunner:Running config idx 4 being 4/19: Scenario 4 of set \"scenario_set1\" for behavior \"IDM\"\n",
      "INFO:BenchmarkRunner:Running config idx 5 being 5/19: Scenario 5 of set \"scenario_set1\" for behavior \"IDM\"\n",
      "INFO:BenchmarkRunner:Running config idx 6 being 6/19: Scenario 6 of set \"scenario_set1\" for behavior \"IDM\"\n",
      "INFO:BenchmarkRunner:Running config idx 7 being 7/19: Scenario 7 of set \"scenario_set1\" for behavior \"IDM\"\n",
      "INFO:BenchmarkRunner:Running config idx 8 being 8/19: Scenario 8 of set \"scenario_set1\" for behavior \"IDM\"\n",
      "INFO:BenchmarkRunner:Running config idx 9 being 9/19: Scenario 9 of set \"scenario_set1\" for behavior \"IDM\"\n",
      "INFO:BenchmarkRunner:\n",
      "------------------- Current Evaluation Results ---------------------- \n",
      " Num. Results:10\n",
      "                         success  collision  max_steps  step\n",
      "behavior scen_set                                          \n",
      "IDM      scenario_set1      0.2        0.8       14.7  13.7 \n",
      " ---------------------------------------------------------------------\n",
      "INFO:BenchmarkRunner:Running config idx 10 being 10/19: Scenario 0 of set \"scenario_set1\" for behavior \"Const\"\n",
      "INFO:BenchmarkRunner:Running config idx 11 being 11/19: Scenario 1 of set \"scenario_set1\" for behavior \"Const\"\n",
      "INFO:BenchmarkRunner:Running config idx 12 being 12/19: Scenario 2 of set \"scenario_set1\" for behavior \"Const\"\n",
      "INFO:BenchmarkRunner:Running config idx 13 being 13/19: Scenario 3 of set \"scenario_set1\" for behavior \"Const\"\n",
      "INFO:BenchmarkRunner:Running config idx 14 being 14/19: Scenario 4 of set \"scenario_set1\" for behavior \"Const\"\n",
      "INFO:BenchmarkRunner:Running config idx 15 being 15/19: Scenario 5 of set \"scenario_set1\" for behavior \"Const\"\n",
      "INFO:BenchmarkRunner:Running config idx 16 being 16/19: Scenario 6 of set \"scenario_set1\" for behavior \"Const\"\n",
      "INFO:BenchmarkRunner:Running config idx 17 being 17/19: Scenario 7 of set \"scenario_set1\" for behavior \"Const\"\n",
      "INFO:BenchmarkRunner:Running config idx 18 being 18/19: Scenario 8 of set \"scenario_set1\" for behavior \"Const\"\n",
      "INFO:BenchmarkRunner:Running config idx 19 being 19/19: Scenario 9 of set \"scenario_set1\" for behavior \"Const\"\n",
      "INFO:BenchmarkRunner:\n",
      "------------------- Current Evaluation Results ---------------------- \n",
      " Num. Results:20\n",
      "                         success  collision  max_steps  step\n",
      "behavior scen_set                                          \n",
      "Const    scenario_set1      0.2        0.8       15.2  14.2\n",
      "IDM      scenario_set1      0.2        0.8       14.7  13.7 \n",
      " ---------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "benchmark_runner = BenchmarkRunner(benchmark_database=db,\\\n",
    "                                   evaluators=evaluators,\\\n",
    "                                   terminal_when=terminal_when,\\\n",
    "                                   behaviors=behaviors_tested,\\\n",
    "                                   log_eval_avg_every=10)\n",
    "\n",
    "result = benchmark_runner.run(maintain_history=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now dump the files, to allow them to be postprocessed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Saved BenchmarkResult to /home/esterle/.cache/bazel/_bazel_esterle/d337abac8c371120c1b9affa1049fa7e/execroot/bark_project/bazel-out/k8-fastbuild/bin/docs/tutorials/run.runfiles/bark_project/docs/tutorials/benchmark_results.zip\n"
     ]
    }
   ],
   "source": [
    "result.dump(os.path.join(\"./benchmark_results.zip\"), dump_configs=True, dump_histories=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Results\n",
    "\n",
    "Benchmark results contain\n",
    "- the evaluated metrics of each simulation run, as a Panda Dataframe\n",
    "- the world state of every simulation (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_loaded = BenchmarkResult.load(os.path.join(\"./benchmark_results.zip\"), load_configs=True, load_histories=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now first analyze the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Terminal</th>\n",
       "      <th>Test1</th>\n",
       "      <th>behavior</th>\n",
       "      <th>collision</th>\n",
       "      <th>config_idx</th>\n",
       "      <th>max_steps</th>\n",
       "      <th>scen_idx</th>\n",
       "      <th>scen_set</th>\n",
       "      <th>step</th>\n",
       "      <th>success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[collision]</td>\n",
       "      <td>200</td>\n",
       "      <td>IDM</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>scenario_set1</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[collision]</td>\n",
       "      <td>200</td>\n",
       "      <td>IDM</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>scenario_set1</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[success]</td>\n",
       "      <td>200</td>\n",
       "      <td>IDM</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>scenario_set1</td>\n",
       "      <td>34</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[collision]</td>\n",
       "      <td>200</td>\n",
       "      <td>IDM</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>scenario_set1</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[collision]</td>\n",
       "      <td>200</td>\n",
       "      <td>IDM</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>scenario_set1</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Terminal  Test1 behavior  collision  config_idx  max_steps  scen_idx  \\\n",
       "0  [collision]    200      IDM       True           0          6         0   \n",
       "1  [collision]    200      IDM       True           1          6         1   \n",
       "2    [success]    200      IDM      False           2         35         2   \n",
       "3  [collision]    200      IDM       True           3         11         3   \n",
       "4  [collision]    200      IDM       True           4         10         4   \n",
       "\n",
       "        scen_set  step  success  \n",
       "0  scenario_set1     5    False  \n",
       "1  scenario_set1     5    False  \n",
       "2  scenario_set1    34     True  \n",
       "3  scenario_set1    10    False  \n",
       "4  scenario_set1     9    False  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = result_loaded.get_data_frame()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Analyzer\n",
    "\n",
    "The benchmark analyzer allows to filter the results to visualize what really happened. These filters can be set via a dictionary with lambda functions specifying the evaluation criteria which must be fullfilled.\n",
    "\n",
    "Let us first load the results into the BenchmarkAnalyzer and then filter the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = BenchmarkAnalyzer(benchmark_result=result_loaded)\n",
    "\n",
    "\n",
    "configs_idm = analyzer.find_configs(criteria={\"behavior\": lambda x: x==\"IDM\", \"collision\": lambda x : x})\n",
    "configs_const = analyzer.find_configs(criteria={\"behavior\": lambda x: x==\"Const\", \"collision\": lambda x : x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 3, 4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs_idm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create a video from them. We will use Matplotlib Viewer and render everything to a video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_step_time=0.2\n",
    "\n",
    "params2 = ParameterServer()\n",
    "\n",
    "fig = plt.figure(figsize=[10, 10])\n",
    "viewer = MPViewer(params=params2, y_length = 80, enforce_y_length=True, enforce_x_length=False,\\\n",
    "                  follow_agent_id=True, axis=fig.gca())\n",
    "video_exporter = VideoRenderer(renderer=viewer, world_step_time=sim_step_time)\n",
    "\n",
    "analyzer.visualize(viewer = video_exporter, real_time_factor = 1, configs_idx_list=configs_idm[1:3], \\\n",
    "                  fontsize=6)\n",
    "                   \n",
    "video_exporter.export_video(filename=\"/tmp/tutorial_video\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
